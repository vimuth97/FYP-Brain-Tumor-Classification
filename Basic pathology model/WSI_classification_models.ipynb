{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WSI classification models.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimuth97/FYP-Brain-Tumor-Classification/blob/main/Basic%20pathology%20model/WSI_classification_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc53yAYqAdQM"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import h5py\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import auc as calc_auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzGwZeSuAkTe"
      },
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lflJ842BpB1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYdcZYO7Bvtd"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/WSI_classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnzHhB-b46UV"
      },
      "source": [
        "def seed_torch(seed=7):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdBzP1BaD9Lb"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvb2S9yDFwvj"
      },
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=20, stop_epoch=50, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 20\n",
        "            stop_epoch (int): Earliest epoch possible for stopping\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.stop_epoch = stop_epoch\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "\n",
        "    def __call__(self, epoch, val_loss, model, ckpt_name = 'checkpoint.pt'):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, ckpt_name)\n",
        "        elif score < self.best_score:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience and epoch > self.stop_epoch:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, ckpt_name)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, ckpt_name):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), ckpt_name)\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTdGFjdpHS5q"
      },
      "source": [
        "def train_loop_clam(epoch, model, loader, optimizer, n_classes, bag_weight, writer = None, loss_fn = None):\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    inst_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    \n",
        "    train_loss = 0.\n",
        "    train_error = 0.\n",
        "    train_inst_loss = 0.\n",
        "    inst_count = 0\n",
        "\n",
        "    print('\\n')\n",
        "    for batch_idx, (data, label) in enumerate(loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        logits, Y_prob, Y_hat, _, instance_dict = model(data, label=label, instance_eval=True)\n",
        "\n",
        "        acc_logger.log(Y_hat, label)\n",
        "        loss = loss_fn(logits, label)\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        instance_loss = instance_dict['instance_loss']\n",
        "        inst_count+=1\n",
        "        instance_loss_value = instance_loss.item()\n",
        "        train_inst_loss += instance_loss_value\n",
        "        \n",
        "        total_loss = bag_weight * loss + (1-bag_weight) * instance_loss \n",
        "\n",
        "        inst_preds = instance_dict['inst_preds']\n",
        "        inst_labels = instance_dict['inst_labels']\n",
        "        inst_logger.log_batch(inst_preds, inst_labels)\n",
        "\n",
        "        train_loss += loss_value\n",
        "        if (batch_idx + 1) % 20 == 0:\n",
        "            print('batch {}, loss: {:.4f}, instance_loss: {:.4f}, weighted_loss: {:.4f}, '.format(batch_idx, loss_value, instance_loss_value, total_loss.item()) + \n",
        "                'label: {}, bag_size: {}'.format(label.item(), data.size(0)))\n",
        "\n",
        "        error = calculate_error(Y_hat, label)\n",
        "        train_error += error\n",
        "        \n",
        "        # backward pass\n",
        "        total_loss.backward()\n",
        "        # step\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # calculate loss and error for epoch\n",
        "    train_loss /= len(loader)\n",
        "    train_error /= len(loader)\n",
        "    \n",
        "    if inst_count > 0:\n",
        "        train_inst_loss /= inst_count\n",
        "        print('\\n')\n",
        "        for i in range(2):\n",
        "            acc, correct, count = inst_logger.get_summary(i)\n",
        "            print('class {} clustering acc {}: correct {}/{}'.format(i, acc, correct, count))\n",
        "\n",
        "    print('Epoch: {}, train_loss: {:.4f}, train_clustering_loss:  {:.4f}, train_error: {:.4f}'.format(epoch, train_loss, train_inst_loss,  train_error))\n",
        "    for i in range(n_classes):\n",
        "        acc, correct, count = acc_logger.get_summary(i)\n",
        "        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n",
        "        if writer and acc is not None:\n",
        "            writer.add_scalar('train/class_{}_acc'.format(i), acc, epoch)\n",
        "\n",
        "    if writer:\n",
        "        writer.add_scalar('train/loss', train_loss, epoch)\n",
        "        writer.add_scalar('train/error', train_error, epoch)\n",
        "        writer.add_scalar('train/clustering_loss', train_inst_loss, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG32EzobHbRz"
      },
      "source": [
        "def train_loop(epoch, model, loader, optimizer, n_classes, writer = None, loss_fn = None):   \n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "    model.train()\n",
        "    acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    train_loss = 0.\n",
        "    train_error = 0.\n",
        "\n",
        "    print('\\n')\n",
        "    for batch_idx, (data, label) in enumerate(loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "\n",
        "        logits, Y_prob, Y_hat, _, _ = model(data)\n",
        "        \n",
        "        acc_logger.log(Y_hat, label)\n",
        "        loss = loss_fn(logits, label)\n",
        "        loss_value = loss.item()\n",
        "        \n",
        "        train_loss += loss_value\n",
        "        if (batch_idx + 1) % 20 == 0:\n",
        "            print('batch {}, loss: {:.4f}, label: {}, bag_size: {}'.format(batch_idx, loss_value, label.item(), data.size(0)))\n",
        "           \n",
        "        error = calculate_error(Y_hat, label)\n",
        "        train_error += error\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # step\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # calculate loss and error for epoch\n",
        "    train_loss /= len(loader)\n",
        "    train_error /= len(loader)\n",
        "\n",
        "    print('Epoch: {}, train_loss: {:.4f}, train_error: {:.4f}'.format(epoch, train_loss, train_error))\n",
        "    for i in range(n_classes):\n",
        "        acc, correct, count = acc_logger.get_summary(i)\n",
        "        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n",
        "        if writer:\n",
        "            writer.add_scalar('train/class_{}_acc'.format(i), acc, epoch)\n",
        "\n",
        "    if writer:\n",
        "        writer.add_scalar('train/loss', train_loss, epoch)\n",
        "        writer.add_scalar('train/error', train_error, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxtNUkZHjrR"
      },
      "source": [
        "class Accuracy_Logger(object):\n",
        "    \"\"\"Accuracy logger\"\"\"\n",
        "    def __init__(self, n_classes):\n",
        "        super(Accuracy_Logger, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        self.data = [{\"count\": 0, \"correct\": 0} for i in range(self.n_classes)]\n",
        "    \n",
        "    def log(self, Y_hat, Y):\n",
        "        Y_hat = int(Y_hat)\n",
        "        Y = int(Y)\n",
        "        self.data[Y][\"count\"] += 1\n",
        "        self.data[Y][\"correct\"] += (Y_hat == Y)\n",
        "    \n",
        "    def log_batch(self, Y_hat, Y):\n",
        "        Y_hat = np.array(Y_hat).astype(int)\n",
        "        Y = np.array(Y).astype(int)\n",
        "        for label_class in np.unique(Y):\n",
        "            cls_mask = Y == label_class\n",
        "            self.data[label_class][\"count\"] += cls_mask.sum()\n",
        "            self.data[label_class][\"correct\"] += (Y_hat[cls_mask] == Y[cls_mask]).sum()\n",
        "    \n",
        "    def get_summary(self, c):\n",
        "        count = self.data[c][\"count\"] \n",
        "        correct = self.data[c][\"correct\"]\n",
        "        \n",
        "        if count == 0: \n",
        "            acc = None\n",
        "        else:\n",
        "            acc = float(correct) / count\n",
        "        \n",
        "        return acc, correct, count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tMbjGQEIEYK"
      },
      "source": [
        "def validate(cur, epoch, model, loader, n_classes, early_stopping = None, writer = None, loss_fn = None, results_dir=None):\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    # loader.dataset.update_mode(True)\n",
        "    val_loss = 0.\n",
        "    val_error = 0.\n",
        "    \n",
        "    prob = np.zeros((len(loader), n_classes))\n",
        "    labels = np.zeros(len(loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(loader):\n",
        "            data, label = data.to(device, non_blocking=True), label.to(device, non_blocking=True)\n",
        "\n",
        "            logits, Y_prob, Y_hat, _, _ = model(data)\n",
        "\n",
        "            acc_logger.log(Y_hat, label)\n",
        "            \n",
        "            loss = loss_fn(logits, label)\n",
        "\n",
        "            prob[batch_idx] = Y_prob.cpu().numpy()\n",
        "            labels[batch_idx] = label.item()\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            error = calculate_error(Y_hat, label)\n",
        "            val_error += error\n",
        "            \n",
        "\n",
        "    val_error /= len(loader)\n",
        "    val_loss /= len(loader)\n",
        "\n",
        "    if n_classes == 2:\n",
        "        auc = roc_auc_score(labels, prob[:, 1])\n",
        "    \n",
        "    else:\n",
        "        auc = roc_auc_score(labels, prob, multi_class='ovr')\n",
        "    \n",
        "    \n",
        "    if writer:\n",
        "        writer.add_scalar('val/loss', val_loss, epoch)\n",
        "        writer.add_scalar('val/auc', auc, epoch)\n",
        "        writer.add_scalar('val/error', val_error, epoch)\n",
        "\n",
        "    print('\\nVal Set, val_loss: {:.4f}, val_error: {:.4f}, auc: {:.4f}'.format(val_loss, val_error, auc))\n",
        "    for i in range(n_classes):\n",
        "        acc, correct, count = acc_logger.get_summary(i)\n",
        "        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))     \n",
        "\n",
        "    if early_stopping:\n",
        "        assert results_dir\n",
        "        early_stopping(epoch, val_loss, model, ckpt_name = os.path.join(results_dir, \"s_{}_checkpoint.pt\".format(cur)))\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-_ohfyAIKAC"
      },
      "source": [
        "def validate_clam(cur, epoch, model, loader, n_classes, early_stopping = None, writer = None, loss_fn = None, results_dir = None):\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    inst_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    val_loss = 0.\n",
        "    val_error = 0.\n",
        "\n",
        "    val_inst_loss = 0.\n",
        "    val_inst_acc = 0.\n",
        "    inst_count=0\n",
        "    \n",
        "    prob = np.zeros((len(loader), n_classes))\n",
        "    labels = np.zeros(len(loader))\n",
        "    sample_size = model.k_sample\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(loader):\n",
        "            data, label = data.to(device), label.to(device)      \n",
        "            logits, Y_prob, Y_hat, _, instance_dict = model(data, label=label, instance_eval=True)\n",
        "            acc_logger.log(Y_hat, label)\n",
        "            \n",
        "            loss = loss_fn(logits, label)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            instance_loss = instance_dict['instance_loss']\n",
        "            \n",
        "            inst_count+=1\n",
        "            instance_loss_value = instance_loss.item()\n",
        "            val_inst_loss += instance_loss_value\n",
        "\n",
        "            inst_preds = instance_dict['inst_preds']\n",
        "            inst_labels = instance_dict['inst_labels']\n",
        "            inst_logger.log_batch(inst_preds, inst_labels)\n",
        "\n",
        "            prob[batch_idx] = Y_prob.cpu().numpy()\n",
        "            labels[batch_idx] = label.item()\n",
        "            \n",
        "            error = calculate_error(Y_hat, label)\n",
        "            val_error += error\n",
        "\n",
        "    val_error /= len(loader)\n",
        "    val_loss /= len(loader)\n",
        "\n",
        "    if n_classes == 2:\n",
        "        auc = roc_auc_score(labels, prob[:, 1])\n",
        "        aucs = []\n",
        "    else:\n",
        "        aucs = []\n",
        "        binary_labels = label_binarize(labels, classes=[i for i in range(n_classes)])\n",
        "        for class_idx in range(n_classes):\n",
        "            if class_idx in labels:\n",
        "                fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], prob[:, class_idx])\n",
        "                aucs.append(calc_auc(fpr, tpr))\n",
        "            else:\n",
        "                aucs.append(float('nan'))\n",
        "\n",
        "        auc = np.nanmean(np.array(aucs))\n",
        "\n",
        "    print('\\nVal Set, val_loss: {:.4f}, val_error: {:.4f}, auc: {:.4f}'.format(val_loss, val_error, auc))\n",
        "    if inst_count > 0:\n",
        "        val_inst_loss /= inst_count\n",
        "        for i in range(2):\n",
        "            acc, correct, count = inst_logger.get_summary(i)\n",
        "            print('class {} clustering acc {}: correct {}/{}'.format(i, acc, correct, count))\n",
        "    \n",
        "    if writer:\n",
        "        writer.add_scalar('val/loss', val_loss, epoch)\n",
        "        writer.add_scalar('val/auc', auc, epoch)\n",
        "        writer.add_scalar('val/error', val_error, epoch)\n",
        "        writer.add_scalar('val/inst_loss', val_inst_loss, epoch)\n",
        "\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        acc, correct, count = acc_logger.get_summary(i)\n",
        "        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n",
        "        \n",
        "        if writer and acc is not None:\n",
        "            writer.add_scalar('val/class_{}_acc'.format(i), acc, epoch)\n",
        "     \n",
        "\n",
        "    if early_stopping:\n",
        "        assert results_dir\n",
        "        early_stopping(epoch, val_loss, model, ckpt_name = os.path.join(results_dir, \"s_{}_checkpoint.pt\".format(cur)))\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoZnz_NzH4Oy"
      },
      "source": [
        "def summary(model, loader, n_classes):\n",
        "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    acc_logger = Accuracy_Logger(n_classes=n_classes)\n",
        "    model.eval()\n",
        "    test_loss = 0.\n",
        "    test_error = 0.\n",
        "\n",
        "    all_probs = np.zeros((len(loader), n_classes))\n",
        "    all_labels = np.zeros(len(loader))\n",
        "\n",
        "    slide_ids = loader.dataset.slide_data['slide_id']\n",
        "    patient_results = {}\n",
        "\n",
        "    for batch_idx, (data, label) in enumerate(loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        slide_id = slide_ids.iloc[batch_idx]\n",
        "        with torch.no_grad():\n",
        "            logits, Y_prob, Y_hat, _, _ = model(data)\n",
        "\n",
        "        acc_logger.log(Y_hat, label)\n",
        "        probs = Y_prob.cpu().numpy()\n",
        "        all_probs[batch_idx] = probs\n",
        "        all_labels[batch_idx] = label.item()\n",
        "        \n",
        "        patient_results.update({slide_id: {'slide_id': np.array(slide_id), 'prob': probs, 'label': label.item()}})\n",
        "        error = calculate_error(Y_hat, label)\n",
        "        test_error += error\n",
        "\n",
        "    test_error /= len(loader)\n",
        "\n",
        "    if n_classes == 2:\n",
        "        auc = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "        aucs = []\n",
        "    else:\n",
        "        aucs = []\n",
        "        binary_labels = label_binarize(all_labels, classes=[i for i in range(n_classes)])\n",
        "        for class_idx in range(n_classes):\n",
        "            if class_idx in all_labels:\n",
        "                fpr, tpr, _ = roc_curve(binary_labels[:, class_idx], all_probs[:, class_idx])\n",
        "                aucs.append(calc_auc(fpr, tpr))\n",
        "            else:\n",
        "                aucs.append(float('nan'))\n",
        "\n",
        "        auc = np.nanmean(np.array(aucs))\n",
        "\n",
        "\n",
        "    return patient_results, test_error, auc, acc_logger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-6jEa4IBYjq"
      },
      "source": [
        "def train(datasets, cur):\n",
        "    \"\"\"   \n",
        "        train for a single fold\n",
        "    \"\"\"\n",
        "    print('\\nTraining Fold {}!'.format(cur))\n",
        "    writer_dir = os.path.join(results_dir, str(cur))\n",
        "    if not os.path.isdir(writer_dir):\n",
        "        os.mkdir(writer_dir)\n",
        "\n",
        "    if log_data:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter(writer_dir, flush_secs=15)\n",
        "\n",
        "    else:\n",
        "        writer = None\n",
        "\n",
        "    print('\\nInit train/val/test splits...', end=' ')\n",
        "    train_split, val_split, test_split = datasets\n",
        "    save_splits(datasets, ['train', 'val', 'test'], os.path.join(results_dir, 'splits_{}.csv'.format(cur)))\n",
        "    print('Done!')\n",
        "    print(\"Training on {} samples\".format(len(train_split)))\n",
        "    print(\"Validating on {} samples\".format(len(val_split)))\n",
        "    print(\"Testing on {} samples\".format(len(test_split)))\n",
        "\n",
        "    print('\\nInit loss function...', end=' ')\n",
        "    if bag_loss == 'svm':\n",
        "        from topk.svm import SmoothTop1SVM\n",
        "        loss_fn = SmoothTop1SVM(n_classes = n_classes)\n",
        "        if device.type == 'cuda':\n",
        "            loss_fn = loss_fn.cuda()\n",
        "    else:\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "    print('Done!')\n",
        "    \n",
        "    print('\\nInit Model...', end=' ')\n",
        "    model_dict = {\"dropout\": drop_out, 'n_classes': n_classes}\n",
        "    if model_type == 'clam' and subtyping:\n",
        "        model_dict.update({'subtyping': True})\n",
        "    \n",
        "    if model_size is not None and model_type != 'mil':\n",
        "        model_dict.update({\"size_arg\": model_size})\n",
        "    \n",
        "    if model_type in ['clam_sb', 'clam_mb']:\n",
        "        if subtyping:\n",
        "            model_dict.update({'subtyping': True})\n",
        "        \n",
        "        if B > 0:\n",
        "            model_dict.update({'k_sample': B})\n",
        "        \n",
        "        if inst_loss == 'svm':\n",
        "            from topk.svm import SmoothTop1SVM\n",
        "            instance_loss_fn = SmoothTop1SVM(n_classes = 2)\n",
        "            if device.type == 'cuda':\n",
        "                instance_loss_fn = instance_loss_fn.cuda()\n",
        "        else:\n",
        "            instance_loss_fn = nn.CrossEntropyLoss()\n",
        "        \n",
        "        if model_type =='clam_sb':\n",
        "            model = CLAM_SB(**model_dict, instance_loss_fn=instance_loss_fn)\n",
        "        elif model_type == 'clam_mb':\n",
        "            model = CLAM_MB(**model_dict, instance_loss_fn=instance_loss_fn)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "    \n",
        "    else: # model_type == 'mil'\n",
        "        if n_classes > 2:\n",
        "            model = MIL_fc_mc(**model_dict)\n",
        "        else:\n",
        "            model = MIL_fc(**model_dict)\n",
        "    \n",
        "    model.relocate()\n",
        "    print('Done!')\n",
        "    print_network(model)\n",
        "\n",
        "    print('\\nInit optimizer ...', end=' ')\n",
        "    optimizer = get_optim(model, args)\n",
        "    print('Done!')\n",
        "    \n",
        "    print('\\nInit Loaders...', end=' ')\n",
        "    train_loader = get_split_loader(train_split, training=True, testing = testing, weighted = weighted_sample)\n",
        "    val_loader = get_split_loader(val_split,  testing = testing)\n",
        "    test_loader = get_split_loader(test_split, testing = testing)\n",
        "    print('Done!')\n",
        "\n",
        "    print('\\nSetup EarlyStopping...', end=' ')\n",
        "    if early_stopping:\n",
        "        early_stopping = EarlyStopping(patience = 20, stop_epoch=50, verbose = True)\n",
        "\n",
        "    else:\n",
        "        early_stopping = None\n",
        "    print('Done!')\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        if model_type in ['clam_sb', 'clam_mb'] and not no_inst_cluster:     \n",
        "            train_loop_clam(epoch, model, train_loader, optimizer, n_classes, bag_weight, writer, loss_fn)\n",
        "            stop = validate_clam(cur, epoch, model, val_loader, n_classes, \n",
        "                early_stopping, writer, loss_fn, results_dir)\n",
        "        \n",
        "        else:\n",
        "            train_loop(epoch, model, train_loader, optimizer, n_classes, writer, loss_fn)\n",
        "            stop = validate(cur, epoch, model, val_loader, n_classes, \n",
        "                early_stopping, writer, loss_fn, results_dir)\n",
        "        \n",
        "        if stop: \n",
        "            break\n",
        "\n",
        "    if early_stopping:\n",
        "        model.load_state_dict(torch.load(os.path.join(results_dir, \"s_{}_checkpoint.pt\".format(cur))))\n",
        "    else:\n",
        "        torch.save(model.state_dict(), os.path.join(results_dir, \"s_{}_checkpoint.pt\".format(cur)))\n",
        "\n",
        "    _, val_error, val_auc, _= summary(model, val_loader, n_classes)\n",
        "    print('Val error: {:.4f}, ROC AUC: {:.4f}'.format(val_error, val_auc))\n",
        "\n",
        "    results_dict, test_error, test_auc, acc_logger = summary(model, test_loader, n_classes)\n",
        "    print('Test error: {:.4f}, ROC AUC: {:.4f}'.format(test_error, test_auc))\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        acc, correct, count = acc_logger.get_summary(i)\n",
        "        print('class {}: acc {}, correct {}/{}'.format(i, acc, correct, count))\n",
        "\n",
        "        if writer:\n",
        "            writer.add_scalar('final/test_class_{}_acc'.format(i), acc, 0)\n",
        "\n",
        "    if writer:\n",
        "        writer.add_scalar('final/val_error', val_error, 0)\n",
        "        writer.add_scalar('final/val_auc', val_auc, 0)\n",
        "        writer.add_scalar('final/test_error', test_error, 0)\n",
        "        writer.add_scalar('final/test_auc', test_auc, 0)\n",
        "        writer.close()\n",
        "    return results_dict, test_auc, val_auc, 1-test_error, 1-val_error "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2iOpbevJaD5"
      },
      "source": [
        "#Dataset Generic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OebFrTBd5ZlK"
      },
      "source": [
        "def save_splits(split_datasets, column_keys, filename, boolean_style=False):\n",
        "\tsplits = [split_datasets[i].slide_data['slide_id'] for i in range(len(split_datasets))]\n",
        "\tif not boolean_style:\n",
        "\t\tdf = pd.concat(splits, ignore_index=True, axis=1)\n",
        "\t\tdf.columns = column_keys\n",
        "\telse:\n",
        "\t\tdf = pd.concat(splits, ignore_index = True, axis=0)\n",
        "\t\tindex = df.values.tolist()\n",
        "\t\tone_hot = np.eye(len(split_datasets)).astype(bool)\n",
        "\t\tbool_array = np.repeat(one_hot, [len(dset) for dset in split_datasets], axis=0)\n",
        "\t\tdf = pd.DataFrame(bool_array, index=index, columns = ['train', 'val', 'test'])\n",
        "\n",
        "\tdf.to_csv(filename)\n",
        "\tprint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOyPmvy9J-l5"
      },
      "source": [
        "class Generic_WSI_Classification_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self,\n",
        "\t\tcsv_path = 'dataset_csv/ccrcc_clean.csv',\n",
        "\t\tshuffle = False, \n",
        "\t\tseed = 7, \n",
        "\t\tprint_info = True,\n",
        "\t\tlabel_dict = {},\n",
        "\t\tfilter_dict = {},\n",
        "\t\tignore=[],\n",
        "\t\tpatient_strat=False,\n",
        "\t\tlabel_col = None,\n",
        "\t\tpatient_voting = 'max',\n",
        "\t\t):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tcsv_file (string): Path to the csv file with annotations.\n",
        "\t\t\tshuffle (boolean): Whether to shuffle\n",
        "\t\t\tseed (int): random seed for shuffling the data\n",
        "\t\t\tprint_info (boolean): Whether to print a summary of the dataset\n",
        "\t\t\tlabel_dict (dict): Dictionary with key, value pairs for converting str labels to int\n",
        "\t\t\tignore (list): List containing class labels to ignore\n",
        "\t\t\"\"\"\n",
        "\t\tself.label_dict = label_dict\n",
        "\t\tself.num_classes = len(set(self.label_dict.values()))\n",
        "\t\tself.seed = seed\n",
        "\t\tself.print_info = print_info\n",
        "\t\tself.patient_strat = patient_strat\n",
        "\t\tself.train_ids, self.val_ids, self.test_ids  = (None, None, None)\n",
        "\t\tself.data_dir = None\n",
        "\t\tif not label_col:\n",
        "\t\t\tlabel_col = 'label'\n",
        "\t\tself.label_col = label_col\n",
        "\n",
        "\t\tslide_data = pd.read_csv(csv_path)\n",
        "\t\tslide_data = self.filter_df(slide_data, filter_dict)\n",
        "\t\tslide_data = self.df_prep(slide_data, self.label_dict, ignore, self.label_col)\n",
        "\n",
        "\t\t###shuffle data\n",
        "\t\tif shuffle:\n",
        "\t\t\tnp.random.seed(seed)\n",
        "\t\t\tnp.random.shuffle(slide_data)\n",
        "\n",
        "\t\tself.slide_data = slide_data\n",
        "\n",
        "\t\tself.patient_data_prep(patient_voting)\n",
        "\t\tself.cls_ids_prep()\n",
        "\n",
        "\t\tif print_info:\n",
        "\t\t\tself.summarize()\n",
        "\n",
        "\tdef cls_ids_prep(self):\n",
        "\t\t# store ids corresponding each class at the patient or case level\n",
        "\t\tself.patient_cls_ids = [[] for i in range(self.num_classes)]\t\t\n",
        "\t\tfor i in range(self.num_classes):\n",
        "\t\t\tself.patient_cls_ids[i] = np.where(self.patient_data['label'] == i)[0]\n",
        "\n",
        "\t\t# store ids corresponding each class at the slide level\n",
        "\t\tself.slide_cls_ids = [[] for i in range(self.num_classes)]\n",
        "\t\tfor i in range(self.num_classes):\n",
        "\t\t\tself.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]\n",
        "\n",
        "\tdef patient_data_prep(self, patient_voting='max'):\n",
        "\t\tpatients = np.unique(np.array(self.slide_data['case_id'])) # get unique patients\n",
        "\t\tpatient_labels = []\n",
        "\t\t\n",
        "\t\tfor p in patients:\n",
        "\t\t\tlocations = self.slide_data[self.slide_data['case_id'] == p].index.tolist()\n",
        "\t\t\tassert len(locations) > 0\n",
        "\t\t\tlabel = self.slide_data['label'][locations].values\n",
        "\t\t\tif patient_voting == 'max':\n",
        "\t\t\t\tlabel = label.max() # get patient label (MIL convention)\n",
        "\t\t\telif patient_voting == 'maj':\n",
        "\t\t\t\tlabel = stats.mode(label)[0]\n",
        "\t\t\telse:\n",
        "\t\t\t\traise NotImplementedError\n",
        "\t\t\tpatient_labels.append(label)\n",
        "\t\t\n",
        "\t\tself.patient_data = {'case_id':patients, 'label':np.array(patient_labels)}\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef df_prep(data, label_dict, ignore, label_col): # map label to sub-class type {'subtype_1':0, 'subtype_2':1, 'subtype_3':2}\n",
        "\t\tif label_col != 'label':\n",
        "\t\t\tdata['label'] = data[label_col].copy()\n",
        "\n",
        "\t\tmask = data['label'].isin(ignore)\n",
        "\t\tdata = data[~mask]\n",
        "\t\tdata.reset_index(drop=True, inplace=True)\n",
        "\t\tfor i in data.index:\n",
        "\t\t\tkey = data.loc[i, 'label']\n",
        "\t\t\tdata.at[i, 'label'] = label_dict[key]\n",
        "\n",
        "\t\treturn data\n",
        "\n",
        "\tdef filter_df(self, df, filter_dict={}):\n",
        "\t\tif len(filter_dict) > 0:\n",
        "\t\t\tfilter_mask = np.full(len(df), True, bool)\n",
        "\t\t\t# assert 'label' not in filter_dict.keys()\n",
        "\t\t\tfor key, val in filter_dict.items():\n",
        "\t\t\t\tmask = df[key].isin(val)\n",
        "\t\t\t\tfilter_mask = np.logical_and(filter_mask, mask)\n",
        "\t\t\tdf = df[filter_mask]\n",
        "\t\treturn df\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\tif self.patient_strat:\n",
        "\t\t\treturn len(self.patient_data['case_id'])\n",
        "\n",
        "\t\telse:\n",
        "\t\t\treturn len(self.slide_data)\n",
        "\n",
        "\tdef summarize(self):\n",
        "\t\tprint(\"label column: {}\".format(self.label_col))\n",
        "\t\tprint(\"label dictionary: {}\".format(self.label_dict))\n",
        "\t\tprint(\"number of classes: {}\".format(self.num_classes))\n",
        "\t\tprint(\"slide-level counts: \", '\\n', self.slide_data['label'].value_counts(sort = False))\n",
        "\t\tfor i in range(self.num_classes):\n",
        "\t\t\tprint('Patient-LVL; Number of samples registered in class %d: %d' % (i, self.patient_cls_ids[i].shape[0]))\n",
        "\t\t\tprint('Slide-LVL; Number of samples registered in class %d: %d' % (i, self.slide_cls_ids[i].shape[0]))\n",
        "\n",
        "\tdef create_splits(self, k = 3, val_num = (25, 25), test_num = (40, 40), label_frac = 1.0, custom_test_ids = None):\n",
        "\t\tsettings = {\n",
        "\t\t\t\t\t'n_splits' : k, \n",
        "\t\t\t\t\t'val_num' : val_num, \n",
        "\t\t\t\t\t'test_num': test_num,\n",
        "\t\t\t\t\t'label_frac': label_frac,\n",
        "\t\t\t\t\t'seed': self.seed,\n",
        "\t\t\t\t\t'custom_test_ids': custom_test_ids\n",
        "\t\t\t\t\t}\n",
        "\n",
        "\t\tif self.patient_strat:\n",
        "\t\t\tsettings.update({'cls_ids' : self.patient_cls_ids, 'samples': len(self.patient_data['case_id'])})\n",
        "\t\telse:\n",
        "\t\t\tsettings.update({'cls_ids' : self.slide_cls_ids, 'samples': len(self.slide_data)})\n",
        "\n",
        "\t\tself.split_gen = generate_split(**settings)\n",
        "\n",
        "\tdef set_splits(self,start_from=None):\n",
        "\t\tif start_from:\n",
        "\t\t\tids = nth(self.split_gen, start_from)\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tids = next(self.split_gen)\n",
        "\n",
        "\t\tif self.patient_strat:\n",
        "\t\t\tslide_ids = [[] for i in range(len(ids))] \n",
        "\n",
        "\t\t\tfor split in range(len(ids)): \n",
        "\t\t\t\tfor idx in ids[split]:\n",
        "\t\t\t\t\tcase_id = self.patient_data['case_id'][idx]\n",
        "\t\t\t\t\tslide_indices = self.slide_data[self.slide_data['case_id'] == case_id].index.tolist()\n",
        "\t\t\t\t\tslide_ids[split].extend(slide_indices)\n",
        "\n",
        "\t\t\tself.train_ids, self.val_ids, self.test_ids = slide_ids[0], slide_ids[1], slide_ids[2]\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tself.train_ids, self.val_ids, self.test_ids = ids\n",
        "\n",
        "\tdef get_split_from_df(self, all_splits, split_key='train'):\n",
        "\t\tsplit = all_splits[split_key]\n",
        "\t\tsplit = split.dropna().reset_index(drop=True)\n",
        "\n",
        "\t\tif len(split) > 0:\n",
        "\t\t\tmask = self.slide_data['slide_id'].isin(split.tolist())\n",
        "\t\t\tdf_slice = self.slide_data[mask].reset_index(drop=True)\n",
        "\t\t\tsplit = Generic_Split(df_slice, data_dir=self.data_dir, num_classes=self.num_classes)\n",
        "\t\telse:\n",
        "\t\t\tsplit = None\n",
        "\t\t\n",
        "\t\treturn split\n",
        "\n",
        "\tdef get_merged_split_from_df(self, all_splits, split_keys=['train']):\n",
        "\t\tmerged_split = []\n",
        "\t\tfor split_key in split_keys:\n",
        "\t\t\tsplit = all_splits[split_key]\n",
        "\t\t\tsplit = split.dropna().reset_index(drop=True).tolist()\n",
        "\t\t\tmerged_split.extend(split)\n",
        "\n",
        "\t\tif len(split) > 0:\n",
        "\t\t\tmask = self.slide_data['slide_id'].isin(merged_split)\n",
        "\t\t\tdf_slice = self.slide_data[mask].reset_index(drop=True)\n",
        "\t\t\tsplit = Generic_Split(df_slice, data_dir=self.data_dir, num_classes=self.num_classes)\n",
        "\t\telse:\n",
        "\t\t\tsplit = None\n",
        "\t\t\n",
        "\t\treturn split\n",
        "\n",
        "\n",
        "\tdef return_splits(self, from_id=True, csv_path=None):\n",
        "\t\tif from_id:\n",
        "\t\t\tif len(self.train_ids) > 0:\n",
        "\t\t\t\ttrain_data = self.slide_data.loc[self.train_ids].reset_index(drop=True)\n",
        "\t\t\t\ttrain_split = Generic_Split(train_data, data_dir=self.data_dir, num_classes=self.num_classes)\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\ttrain_split = None\n",
        "\t\t\t\n",
        "\t\t\tif len(self.val_ids) > 0:\n",
        "\t\t\t\tval_data = self.slide_data.loc[self.val_ids].reset_index(drop=True)\n",
        "\t\t\t\tval_split = Generic_Split(val_data, data_dir=self.data_dir, num_classes=self.num_classes)\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tval_split = None\n",
        "\t\t\t\n",
        "\t\t\tif len(self.test_ids) > 0:\n",
        "\t\t\t\ttest_data = self.slide_data.loc[self.test_ids].reset_index(drop=True)\n",
        "\t\t\t\ttest_split = Generic_Split(test_data, data_dir=self.data_dir, num_classes=self.num_classes)\n",
        "\t\t\t\n",
        "\t\t\telse:\n",
        "\t\t\t\ttest_split = None\n",
        "\t\t\t\n",
        "\t\t\n",
        "\t\telse:\n",
        "\t\t\tassert csv_path \n",
        "\t\t\tall_splits = pd.read_csv(csv_path)\n",
        "\t\t\ttrain_split = self.get_split_from_df(all_splits, 'train')\n",
        "\t\t\tval_split = self.get_split_from_df(all_splits, 'val')\n",
        "\t\t\ttest_split = self.get_split_from_df(all_splits, 'test')\n",
        "\t\t\t\n",
        "\t\treturn train_split, val_split, test_split\n",
        "\n",
        "\tdef get_list(self, ids):\n",
        "\t\treturn self.slide_data['slide_id'][ids]\n",
        "\n",
        "\tdef getlabel(self, ids):\n",
        "\t\treturn self.slide_data['label'][ids]\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn None\n",
        "\n",
        "\tdef test_split_gen(self, return_descriptor=False):\n",
        "\n",
        "\t\tif return_descriptor:\n",
        "\t\t\tindex = [list(self.label_dict.keys())[list(self.label_dict.values()).index(i)] for i in range(self.num_classes)]\n",
        "\t\t\tcolumns = ['train', 'val', 'test']\n",
        "\t\t\tdf = pd.DataFrame(np.full((len(index), len(columns)), 0, dtype=np.int32), index= index,\n",
        "\t\t\t\t\t\t\tcolumns= columns)\n",
        "\n",
        "\t\tcount = len(self.train_ids)\n",
        "\t\tprint('\\nnumber of training samples: {}'.format(count))\n",
        "\t\tlabels = self.getlabel(self.train_ids)\n",
        "\t\tunique, counts = np.unique(labels, return_counts=True)\n",
        "\t\tfor u in range(len(unique)):\n",
        "\t\t\tprint('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n",
        "\t\t\tif return_descriptor:\n",
        "\t\t\t\tdf.loc[index[u], 'train'] = counts[u]\n",
        "\t\t\n",
        "\t\tcount = len(self.val_ids)\n",
        "\t\tprint('\\nnumber of val samples: {}'.format(count))\n",
        "\t\tlabels = self.getlabel(self.val_ids)\n",
        "\t\tunique, counts = np.unique(labels, return_counts=True)\n",
        "\t\tfor u in range(len(unique)):\n",
        "\t\t\tprint('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n",
        "\t\t\tif return_descriptor:\n",
        "\t\t\t\tdf.loc[index[u], 'val'] = counts[u]\n",
        "\n",
        "\t\tcount = len(self.test_ids)\n",
        "\t\tprint('\\nnumber of test samples: {}'.format(count))\n",
        "\t\tlabels = self.getlabel(self.test_ids)\n",
        "\t\tunique, counts = np.unique(labels, return_counts=True)\n",
        "\t\tfor u in range(len(unique)):\n",
        "\t\t\tprint('number of samples in cls {}: {}'.format(unique[u], counts[u]))\n",
        "\t\t\tif return_descriptor:\n",
        "\t\t\t\tdf.loc[index[u], 'test'] = counts[u]\n",
        "\n",
        "\t\tassert len(np.intersect1d(self.train_ids, self.test_ids)) == 0\n",
        "\t\tassert len(np.intersect1d(self.train_ids, self.val_ids)) == 0\n",
        "\t\tassert len(np.intersect1d(self.val_ids, self.test_ids)) == 0\n",
        "\n",
        "\t\tif return_descriptor:\n",
        "\t\t\treturn df\n",
        "\n",
        "\tdef save_split(self, filename):\n",
        "\t\ttrain_split = self.get_list(self.train_ids)\n",
        "\t\tval_split = self.get_list(self.val_ids)\n",
        "\t\ttest_split = self.get_list(self.test_ids)\n",
        "\t\tdf_tr = pd.DataFrame({'train': train_split})\n",
        "\t\tdf_v = pd.DataFrame({'val': val_split})\n",
        "\t\tdf_t = pd.DataFrame({'test': test_split})\n",
        "\t\tdf = pd.concat([df_tr, df_v, df_t], axis=1) \n",
        "\t\tdf.to_csv(filename, index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r71DW2V4Jn7a"
      },
      "source": [
        "class Generic_MIL_Dataset(Generic_WSI_Classification_Dataset):\n",
        "\tdef __init__(self,\n",
        "\t\tdata_dir, \n",
        "\t\t**kwargs):\n",
        "\t\n",
        "\t\tsuper(Generic_MIL_Dataset, self).__init__(**kwargs)\n",
        "\t\tself.data_dir = data_dir\n",
        "\t\tself.use_h5 = False\n",
        "\n",
        "\tdef load_from_h5(self, toggle):\n",
        "\t\tself.use_h5 = toggle\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tslide_id = self.slide_data['slide_id'][idx]\n",
        "\t\tlabel = self.slide_data['label'][idx]\n",
        "\t\tif type(self.data_dir) == dict:\n",
        "\t\t\tsource = self.slide_data['source'][idx]\n",
        "\t\t\tdata_dir = self.data_dir[source]\n",
        "\t\telse:\n",
        "\t\t\tdata_dir = self.data_dir\n",
        "\n",
        "\t\tif not self.use_h5:\n",
        "\t\t\tif self.data_dir:\n",
        "\t\t\t\tfull_path = os.path.join(data_dir, 'pt_files', '{}.pt'.format(slide_id))\n",
        "\t\t\t\tfeatures = torch.load(full_path)\n",
        "\t\t\t\treturn features, label\n",
        "\t\t\t\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn slide_id, label\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tfull_path = os.path.join(data_dir,'h5_files','{}.h5'.format(slide_id))\n",
        "\t\t\twith h5py.File(full_path,'r') as hdf5_file:\n",
        "\t\t\t\tfeatures = hdf5_file['features'][:]\n",
        "\t\t\t\tcoords = hdf5_file['coords'][:]\n",
        "\n",
        "\t\t\tfeatures = torch.from_numpy(features)\n",
        "\t\t\treturn features, label, coords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtff2XR6Kain"
      },
      "source": [
        "class Generic_Split(Generic_MIL_Dataset):\n",
        "\tdef __init__(self, slide_data, data_dir=None, num_classes=2):\n",
        "\t\tself.use_h5 = False\n",
        "\t\tself.slide_data = slide_data\n",
        "\t\tself.data_dir = data_dir\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\tself.slide_cls_ids = [[] for i in range(self.num_classes)]\n",
        "\t\tfor i in range(self.num_classes):\n",
        "\t\t\tself.slide_cls_ids[i] = np.where(self.slide_data['label'] == i)[0]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.slide_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzFUciIvK6vr"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enYdOpme-WR4"
      },
      "source": [
        "def calculate_error(Y_hat, Y):\n",
        "\terror = 1. - Y_hat.float().eq(Y.float()).float().mean().item()\n",
        "\n",
        "\treturn error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3hzZxoi-LrJ"
      },
      "source": [
        "def print_network(net):\n",
        "\tnum_params = 0\n",
        "\tnum_params_train = 0\n",
        "\tprint(net)\n",
        "\t\n",
        "\tfor param in net.parameters():\n",
        "\t\tn = param.numel()\n",
        "\t\tnum_params += n\n",
        "\t\tif param.requires_grad:\n",
        "\t\t\tnum_params_train += n\n",
        "\t\n",
        "\tprint('Total number of parameters: %d' % num_params)\n",
        "\tprint('Total number of trainable parameters: %d' % num_params_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06IDyFzB95JA"
      },
      "source": [
        "def get_optim(model, args):\n",
        "\tif args.opt == \"adam\":\n",
        "\t\toptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, weight_decay=args.reg)\n",
        "\telif args.opt == 'sgd':\n",
        "\t\toptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=0.9, weight_decay=args.reg)\n",
        "\telse:\n",
        "\t\traise NotImplementedError\n",
        "\treturn optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1ro4Kf89tpz"
      },
      "source": [
        "def get_split_loader(split_dataset, training = False, testing = False, weighted = False):\n",
        "\t\"\"\"\n",
        "\t\treturn either the validation loader or training loader \n",
        "\t\"\"\"\n",
        "\tkwargs = {'num_workers': 4} if device.type == \"cuda\" else {}\n",
        "\tif not testing:\n",
        "\t\tif training:\n",
        "\t\t\tif weighted:\n",
        "\t\t\t\tweights = make_weights_for_balanced_classes_split(split_dataset)\n",
        "\t\t\t\tloader = DataLoader(split_dataset, batch_size=1, sampler = WeightedRandomSampler(weights, len(weights)), collate_fn = collate_MIL, **kwargs)\t\n",
        "\t\t\telse:\n",
        "\t\t\t\tloader = DataLoader(split_dataset, batch_size=1, sampler = RandomSampler(split_dataset), collate_fn = collate_MIL, **kwargs)\n",
        "\t\telse:\n",
        "\t\t\tloader = DataLoader(split_dataset, batch_size=1, sampler = SequentialSampler(split_dataset), collate_fn = collate_MIL, **kwargs)\n",
        "\t\n",
        "\telse:\n",
        "\t\tids = np.random.choice(np.arange(len(split_dataset), int(len(split_dataset)*0.1)), replace = False)\n",
        "\t\tloader = DataLoader(split_dataset, batch_size=1, sampler = SubsetSequentialSampler(ids), collate_fn = collate_MIL, **kwargs )\n",
        "\n",
        "\treturn loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csq6S4RFK9Zn"
      },
      "source": [
        "def save_pkl(filename, save_object):\n",
        "\twriter = open(filename,'wb')\n",
        "\tpickle.dump(save_object, writer)\n",
        "\twriter.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSzGEor76H8K"
      },
      "source": [
        "def nth(iterator, n, default=None):\n",
        "\tif n is None:\n",
        "\t\treturn collections.deque(iterator, maxlen=0)\n",
        "\telse:\n",
        "\t\treturn next(islice(iterator,n, None), default)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ6D4QJn54EI"
      },
      "source": [
        "def generate_split(cls_ids, val_num, test_num, samples, n_splits = 5,\n",
        "\tseed = 7, label_frac = 1.0, custom_test_ids = None):\n",
        "\tindices = np.arange(samples).astype(int)\n",
        "\t\n",
        "\tif custom_test_ids is not None:\n",
        "\t\tindices = np.setdiff1d(indices, custom_test_ids)\n",
        "\n",
        "\tnp.random.seed(seed)\n",
        "\tfor i in range(n_splits):\n",
        "\t\tall_val_ids = []\n",
        "\t\tall_test_ids = []\n",
        "\t\tsampled_train_ids = []\n",
        "\t\t\n",
        "\t\tif custom_test_ids is not None: # pre-built test split, do not need to sample\n",
        "\t\t\tall_test_ids.extend(custom_test_ids)\n",
        "\n",
        "\t\tfor c in range(len(val_num)):\n",
        "\t\t\tpossible_indices = np.intersect1d(cls_ids[c], indices) #all indices of this class\n",
        "\t\t\tval_ids = np.random.choice(possible_indices, val_num[c], replace = False) # validation ids\n",
        "\n",
        "\t\t\tremaining_ids = np.setdiff1d(possible_indices, val_ids) #indices of this class left after validation\n",
        "\t\t\tall_val_ids.extend(val_ids)\n",
        "\n",
        "\t\t\tif custom_test_ids is None: # sample test split\n",
        "\n",
        "\t\t\t\ttest_ids = np.random.choice(remaining_ids, test_num[c], replace = False)\n",
        "\t\t\t\tremaining_ids = np.setdiff1d(remaining_ids, test_ids)\n",
        "\t\t\t\tall_test_ids.extend(test_ids)\n",
        "\n",
        "\t\t\tif label_frac == 1:\n",
        "\t\t\t\tsampled_train_ids.extend(remaining_ids)\n",
        "\t\t\t\n",
        "\t\t\telse:\n",
        "\t\t\t\tsample_num  = math.ceil(len(remaining_ids) * label_frac)\n",
        "\t\t\t\tslice_ids = np.arange(sample_num)\n",
        "\t\t\t\tsampled_train_ids.extend(remaining_ids[slice_ids])\n",
        "\n",
        "\t\tyield sampled_train_ids, all_val_ids, all_test_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjOTFVVM8s0m"
      },
      "source": [
        "def initialize_weights(module):\n",
        "\tfor m in module.modules():\n",
        "\t\tif isinstance(m, nn.Linear):\n",
        "\t\t\tnn.init.xavier_normal_(m.weight)\n",
        "\t\t\tm.bias.data.zero_()\n",
        "\t\t\n",
        "\t\telif isinstance(m, nn.BatchNorm1d):\n",
        "\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\tnn.init.constant_(m.bias, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPjPyNGU7ZRu"
      },
      "source": [
        "#CLAM Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW8hNzgQ6eh5"
      },
      "source": [
        "\"\"\"\n",
        "Attention Network without Gating (2 fc layers)\n",
        "args:\n",
        "    L: input feature dimension\n",
        "    D: hidden layer dimension\n",
        "    dropout: whether to use dropout (p = 0.25)\n",
        "    n_classes: number of classes \n",
        "\"\"\"\n",
        "class Attn_Net(nn.Module):\n",
        "\n",
        "    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n",
        "        super(Attn_Net, self).__init__()\n",
        "        self.module = [\n",
        "            nn.Linear(L, D),\n",
        "            nn.Tanh()]\n",
        "\n",
        "        if dropout:\n",
        "            self.module.append(nn.Dropout(0.25))\n",
        "\n",
        "        self.module.append(nn.Linear(D, n_classes))\n",
        "        \n",
        "        self.module = nn.Sequential(*self.module)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.module(x), x # N x n_classes\n",
        "\n",
        "\"\"\"\n",
        "Attention Network with Sigmoid Gating (3 fc layers)\n",
        "args:\n",
        "    L: input feature dimension\n",
        "    D: hidden layer dimension\n",
        "    dropout: whether to use dropout (p = 0.25)\n",
        "    n_classes: number of classes \n",
        "\"\"\"\n",
        "\n",
        "class Attn_Net_Gated(nn.Module):\n",
        "    def __init__(self, L = 1024, D = 256, dropout = False, n_classes = 1):\n",
        "        super(Attn_Net_Gated, self).__init__()\n",
        "        self.attention_a = [\n",
        "            nn.Linear(L, D),\n",
        "            nn.Tanh()]\n",
        "        \n",
        "        self.attention_b = [nn.Linear(L, D),\n",
        "                            nn.Sigmoid()]\n",
        "        if dropout:\n",
        "            self.attention_a.append(nn.Dropout(0.25))\n",
        "            self.attention_b.append(nn.Dropout(0.25))\n",
        "\n",
        "        self.attention_a = nn.Sequential(*self.attention_a)\n",
        "        self.attention_b = nn.Sequential(*self.attention_b)\n",
        "        \n",
        "        self.attention_c = nn.Linear(D, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a = self.attention_a(x)\n",
        "        b = self.attention_b(x)\n",
        "        A = a.mul(b)\n",
        "        A = self.attention_c(A)  # N x n_classes\n",
        "        return A, x\n",
        "\n",
        "\"\"\"\n",
        "args:\n",
        "    gate: whether to use gated attention network\n",
        "    size_arg: config for network size\n",
        "    dropout: whether to use dropout\n",
        "    k_sample: number of positive/neg patches to sample for instance-level training\n",
        "    dropout: whether to use dropout (p = 0.25)\n",
        "    n_classes: number of classes \n",
        "    instance_loss_fn: loss function to supervise instance-level training\n",
        "    subtyping: whether it's a subtyping problem\n",
        "\"\"\"\n",
        "class CLAM_SB(nn.Module):\n",
        "    def __init__(self, gate = True, size_arg = \"small\", dropout = False, k_sample=8, n_classes=2,\n",
        "        instance_loss_fn=nn.CrossEntropyLoss(), subtyping=False):\n",
        "        super(CLAM_SB, self).__init__()\n",
        "        self.size_dict = {\"small\": [1024, 512, 256], \"big\": [1024, 512, 384]}\n",
        "        size = self.size_dict[size_arg]\n",
        "        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n",
        "        if dropout:\n",
        "            fc.append(nn.Dropout(0.25))\n",
        "        if gate:\n",
        "            attention_net = Attn_Net_Gated(L = size[1], D = size[2], dropout = dropout, n_classes = 1)\n",
        "        else:\n",
        "            attention_net = Attn_Net(L = size[1], D = size[2], dropout = dropout, n_classes = 1)\n",
        "        fc.append(attention_net)\n",
        "        self.attention_net = nn.Sequential(*fc)\n",
        "        self.classifiers = nn.Linear(size[1], n_classes)\n",
        "        instance_classifiers = [nn.Linear(size[1], 2) for i in range(n_classes)]\n",
        "        self.instance_classifiers = nn.ModuleList(instance_classifiers)\n",
        "        self.k_sample = k_sample\n",
        "        self.instance_loss_fn = instance_loss_fn\n",
        "        self.n_classes = n_classes\n",
        "        self.subtyping = subtyping\n",
        "\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def relocate(self):\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.attention_net = self.attention_net.to(device)\n",
        "        self.classifiers = self.classifiers.to(device)\n",
        "        self.instance_classifiers = self.instance_classifiers.to(device)\n",
        "    \n",
        "    @staticmethod\n",
        "    def create_positive_targets(length, device):\n",
        "        return torch.full((length, ), 1, device=device).long()\n",
        "    @staticmethod\n",
        "    def create_negative_targets(length, device):\n",
        "        return torch.full((length, ), 0, device=device).long()\n",
        "    \n",
        "    #instance-level evaluation for in-the-class attention branch\n",
        "    def inst_eval(self, A, h, classifier): \n",
        "        device=h.device\n",
        "        if len(A.shape) == 1:\n",
        "            A = A.view(1, -1)\n",
        "        top_p_ids = torch.topk(A, self.k_sample)[1][-1]\n",
        "        top_p = torch.index_select(h, dim=0, index=top_p_ids)\n",
        "        top_n_ids = torch.topk(-A, self.k_sample, dim=1)[1][-1]\n",
        "        top_n = torch.index_select(h, dim=0, index=top_n_ids)\n",
        "        p_targets = self.create_positive_targets(self.k_sample, device)\n",
        "        n_targets = self.create_negative_targets(self.k_sample, device)\n",
        "\n",
        "        all_targets = torch.cat([p_targets, n_targets], dim=0)\n",
        "        all_instances = torch.cat([top_p, top_n], dim=0)\n",
        "        logits = classifier(all_instances)\n",
        "        all_preds = torch.topk(logits, 1, dim = 1)[1].squeeze(1)\n",
        "        instance_loss = self.instance_loss_fn(logits, all_targets)\n",
        "        return instance_loss, all_preds, all_targets\n",
        "    \n",
        "    #instance-level evaluation for out-of-the-class attention branch\n",
        "    def inst_eval_out(self, A, h, classifier):\n",
        "        device=h.device\n",
        "        if len(A.shape) == 1:\n",
        "            A = A.view(1, -1)\n",
        "        top_p_ids = torch.topk(A, self.k_sample)[1][-1]\n",
        "        top_p = torch.index_select(h, dim=0, index=top_p_ids)\n",
        "        p_targets = self.create_negative_targets(self.k_sample, device)\n",
        "        logits = classifier(top_p)\n",
        "        p_preds = torch.topk(logits, 1, dim = 1)[1].squeeze(1)\n",
        "        instance_loss = self.instance_loss_fn(logits, p_targets)\n",
        "        return instance_loss, p_preds, p_targets\n",
        "\n",
        "    def forward(self, h, label=None, instance_eval=False, return_features=False, attention_only=False):\n",
        "        device = h.device\n",
        "        A, h = self.attention_net(h)  # NxK        \n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        if attention_only:\n",
        "            return A\n",
        "        A_raw = A\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        if instance_eval:\n",
        "            total_inst_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "            inst_labels = F.one_hot(label, num_classes=self.n_classes).squeeze() #binarize label\n",
        "            for i in range(len(self.instance_classifiers)):\n",
        "                inst_label = inst_labels[i].item()\n",
        "                classifier = self.instance_classifiers[i]\n",
        "                if inst_label == 1: #in-the-class:\n",
        "                    instance_loss, preds, targets = self.inst_eval(A, h, classifier)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_targets.extend(targets.cpu().numpy())\n",
        "                else: #out-of-the-class\n",
        "                    if self.subtyping:\n",
        "                        instance_loss, preds, targets = self.inst_eval_out(A, h, classifier)\n",
        "                        all_preds.extend(preds.cpu().numpy())\n",
        "                        all_targets.extend(targets.cpu().numpy())\n",
        "                    else:\n",
        "                        continue\n",
        "                total_inst_loss += instance_loss\n",
        "\n",
        "            if self.subtyping:\n",
        "                total_inst_loss /= len(self.instance_classifiers)\n",
        "                \n",
        "        M = torch.mm(A, h) \n",
        "        logits = self.classifiers(M)\n",
        "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
        "        Y_prob = F.softmax(logits, dim = 1)\n",
        "        if instance_eval:\n",
        "            results_dict = {'instance_loss': total_inst_loss, 'inst_labels': np.array(all_targets), \n",
        "            'inst_preds': np.array(all_preds)}\n",
        "        else:\n",
        "            results_dict = {}\n",
        "        if return_features:\n",
        "            results_dict.update({'features': M})\n",
        "        return logits, Y_prob, Y_hat, A_raw, results_dict\n",
        "\n",
        "class CLAM_MB(CLAM_SB):\n",
        "    def __init__(self, gate = True, size_arg = \"small\", dropout = False, k_sample=8, n_classes=2,\n",
        "        instance_loss_fn=nn.CrossEntropyLoss(), subtyping=False):\n",
        "        nn.Module.__init__(self)\n",
        "        self.size_dict = {\"small\": [1024, 512, 256], \"big\": [1024, 512, 384]}\n",
        "        size = self.size_dict[size_arg]\n",
        "        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n",
        "        if dropout:\n",
        "            fc.append(nn.Dropout(0.25))\n",
        "        if gate:\n",
        "            attention_net = Attn_Net_Gated(L = size[1], D = size[2], dropout = dropout, n_classes = n_classes)\n",
        "        else:\n",
        "            attention_net = Attn_Net(L = size[1], D = size[2], dropout = dropout, n_classes = n_classes)\n",
        "        fc.append(attention_net)\n",
        "        self.attention_net = nn.Sequential(*fc)\n",
        "        bag_classifiers = [nn.Linear(size[1], 1) for i in range(n_classes)] #use an indepdent linear layer to predict each class\n",
        "        self.classifiers = nn.ModuleList(bag_classifiers)\n",
        "        instance_classifiers = [nn.Linear(size[1], 2) for i in range(n_classes)]\n",
        "        self.instance_classifiers = nn.ModuleList(instance_classifiers)\n",
        "        self.k_sample = k_sample\n",
        "        self.instance_loss_fn = instance_loss_fn\n",
        "        self.n_classes = n_classes\n",
        "        self.subtyping = subtyping\n",
        "        initialize_weights(self)\n",
        "\n",
        "    def forward(self, h, label=None, instance_eval=False, return_features=False, attention_only=False):\n",
        "        device = h.device\n",
        "        A, h = self.attention_net(h)  # NxK        \n",
        "        A = torch.transpose(A, 1, 0)  # KxN\n",
        "        if attention_only:\n",
        "            return A\n",
        "        A_raw = A\n",
        "        A = F.softmax(A, dim=1)  # softmax over N\n",
        "\n",
        "        if instance_eval:\n",
        "            total_inst_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "            inst_labels = F.one_hot(label, num_classes=self.n_classes).squeeze() #binarize label\n",
        "            for i in range(len(self.instance_classifiers)):\n",
        "                inst_label = inst_labels[i].item()\n",
        "                classifier = self.instance_classifiers[i]\n",
        "                if inst_label == 1: #in-the-class:\n",
        "                    instance_loss, preds, targets = self.inst_eval(A[i], h, classifier)\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_targets.extend(targets.cpu().numpy())\n",
        "                else: #out-of-the-class\n",
        "                    if self.subtyping:\n",
        "                        instance_loss, preds, targets = self.inst_eval_out(A[i], h, classifier)\n",
        "                        all_preds.extend(preds.cpu().numpy())\n",
        "                        all_targets.extend(targets.cpu().numpy())\n",
        "                    else:\n",
        "                        continue\n",
        "                total_inst_loss += instance_loss\n",
        "\n",
        "            if self.subtyping:\n",
        "                total_inst_loss /= len(self.instance_classifiers)\n",
        "\n",
        "        M = torch.mm(A, h) \n",
        "        logits = torch.empty(1, self.n_classes).float().to(device)\n",
        "        for c in range(self.n_classes):\n",
        "            logits[0, c] = self.classifiers[c](M[c])\n",
        "        Y_hat = torch.topk(logits, 1, dim = 1)[1]\n",
        "        Y_prob = F.softmax(logits, dim = 1)\n",
        "        if instance_eval:\n",
        "            results_dict = {'instance_loss': total_inst_loss, 'inst_labels': np.array(all_targets), \n",
        "            'inst_preds': np.array(all_preds)}\n",
        "        else:\n",
        "            results_dict = {}\n",
        "        if return_features:\n",
        "            results_dict.update({'features': M})\n",
        "        return logits, Y_prob, Y_hat, A_raw, results_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhGctTm8LjQ"
      },
      "source": [
        "#MIL Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFJUkSTa8bi-"
      },
      "source": [
        "class MIL_fc(nn.Module):\n",
        "    def __init__(self, gate = True, size_arg = \"small\", dropout = False, n_classes = 2, top_k=1):\n",
        "        super(MIL_fc, self).__init__()\n",
        "        assert n_classes == 2\n",
        "        self.size_dict = {\"small\": [1024, 512]}\n",
        "        size = self.size_dict[size_arg]\n",
        "        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n",
        "        if dropout:\n",
        "            fc.append(nn.Dropout(0.25))\n",
        "\n",
        "        fc.append(nn.Linear(size[1], n_classes))\n",
        "        self.classifier= nn.Sequential(*fc)\n",
        "        initialize_weights(self)\n",
        "        self.top_k=top_k\n",
        "\n",
        "    def relocate(self):\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.classifier.to(device)\n",
        "\n",
        "    def forward(self, h, return_features=False):\n",
        "        if return_features:\n",
        "            h = self.classifier.module[:3](h)\n",
        "            logits = self.classifier.module[3](h)\n",
        "        else:\n",
        "            logits  = self.classifier(h) # K x 1\n",
        "        \n",
        "        y_probs = F.softmax(logits, dim = 1)\n",
        "        top_instance_idx = torch.topk(y_probs[:, 1], self.top_k, dim=0)[1].view(1,)\n",
        "        top_instance = torch.index_select(logits, dim=0, index=top_instance_idx)\n",
        "        Y_hat = torch.topk(top_instance, 1, dim = 1)[1]\n",
        "        Y_prob = F.softmax(top_instance, dim = 1) \n",
        "        results_dict = {}\n",
        "\n",
        "        if return_features:\n",
        "            top_features = torch.index_select(h, dim=0, index=top_instance_idx)\n",
        "            results_dict.update({'features': top_features})\n",
        "        return top_instance, Y_prob, Y_hat, y_probs, results_dict\n",
        "\n",
        "\n",
        "class MIL_fc_mc(nn.Module):\n",
        "    def __init__(self, gate = True, size_arg = \"small\", dropout = False, n_classes = 2, top_k=1):\n",
        "        super(MIL_fc_mc, self).__init__()\n",
        "        assert n_classes > 2\n",
        "        self.size_dict = {\"small\": [1024, 512]}\n",
        "        size = self.size_dict[size_arg]\n",
        "        fc = [nn.Linear(size[0], size[1]), nn.ReLU()]\n",
        "        if dropout:\n",
        "            fc.append(nn.Dropout(0.25))\n",
        "        self.fc = nn.Sequential(*fc)\n",
        "\n",
        "        self.classifiers = nn.ModuleList([nn.Linear(size[1], 1) for i in range(n_classes)])\n",
        "        initialize_weights(self)\n",
        "        self.top_k=top_k\n",
        "        self.n_classes = n_classes\n",
        "        assert self.top_k == 1\n",
        "\n",
        "    def relocate(self):\n",
        "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.fc = self.fc.to(device)\n",
        "        self.classifiers = self.classifiers.to(device)\n",
        "    \n",
        "    def forward(self, h, return_features=False):\n",
        "        device = h.device\n",
        "       \n",
        "        h = self.fc(h)\n",
        "        logits = torch.empty(h.size(0), self.n_classes).float().to(device)\n",
        "\n",
        "        for c in range(self.n_classes):\n",
        "            if isinstance(self.classifiers, nn.DataParallel):\n",
        "                logits[:, c] = self.classifiers.module[c](h).squeeze(1)\n",
        "            else:\n",
        "                logits[:, c] = self.classifiers[c](h).squeeze(1)        \n",
        "\n",
        "        y_probs = F.softmax(logits, dim = 1)\n",
        "        m = y_probs.view(1, -1).argmax(1)\n",
        "        top_indices = torch.cat(((m // self.n_classes).view(-1, 1), (m % self.n_classes).view(-1, 1)), dim=1).view(-1, 1)\n",
        "        top_instance = logits[top_indices[0]]\n",
        "\n",
        "        Y_hat = top_indices[1]\n",
        "        Y_prob = y_probs[top_indices[0]]\n",
        "        \n",
        "        results_dict = {}\n",
        "\n",
        "        if return_features:\n",
        "            top_features = torch.index_select(h, dim=0, index=top_indices[0])\n",
        "            results_dict.update({'features': top_features})\n",
        "        return top_instance, Y_prob, Y_hat, y_probs, results_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qqiQhE1ITBM"
      },
      "source": [
        "#Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZu1H0Tu5UCc"
      },
      "source": [
        "def main():\n",
        "    # create results directory if necessary\n",
        "    if not os.path.isdir(results_dir):\n",
        "        os.mkdir(results_dir)\n",
        "\n",
        "    if k_start == -1:\n",
        "        start = 0\n",
        "    else:\n",
        "        start = k_start\n",
        "    if k_end == -1:\n",
        "        end = k\n",
        "    else:\n",
        "        end = k_end\n",
        "\n",
        "    all_test_auc = []\n",
        "    all_val_auc = []\n",
        "    all_test_acc = []\n",
        "    all_val_acc = []\n",
        "    folds = np.arange(start, end)\n",
        "    for i in folds:\n",
        "        seed_torch(seed)\n",
        "        train_dataset, val_dataset, test_dataset = dataset.return_splits(from_id=False, \n",
        "                csv_path='{}/splits_{}.csv'.format(split_dir, i))\n",
        "\n",
        "        datasets = (train_dataset, val_dataset, test_dataset)\n",
        "        results, test_auc, val_auc, test_acc, val_acc  = train(datasets, i)\n",
        "        all_test_auc.append(test_auc)\n",
        "        all_val_auc.append(val_auc)\n",
        "        all_test_acc.append(test_acc)\n",
        "        all_val_acc.append(val_acc)\n",
        "        #write results to pkl\n",
        "        filename = os.path.join(results_dir, 'split_{}_results.pkl'.format(i))\n",
        "        save_pkl(filename, results)\n",
        "        \n",
        "    final_df = pd.DataFrame({'folds': folds, 'test_auc': all_test_auc, \n",
        "        'val_auc': all_val_auc, 'test_acc': all_test_acc, 'val_acc' : all_val_acc})\n",
        "\n",
        "    if len(folds) != k:\n",
        "        save_name = 'summary_partial_{}_{}.csv'.format(start, end)\n",
        "    else:\n",
        "        save_name = 'summary.csv'\n",
        "    final_df.to_csv(os.path.join(results_dir, save_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Koxb7Bg94re3"
      },
      "source": [
        "#Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbaPcFTp4qkH"
      },
      "source": [
        "k = 10 #number of folds\n",
        "k_start = 0 #starting fold: -1 - last fold\n",
        "k_end = -1 #ending fold: -1 - first fold\n",
        "encoding_size = 1024\n",
        "n_classes = 3\n",
        "seed = 1\n",
        "model_type = 'clam_sb' # ['clam_sb', 'clam_mb', 'mil']\n",
        "bag_weight = 0.7 \n",
        "inst_loss = None # ['svm', 'ce', None]\n",
        "B = 8\n",
        "data_root_dir = \"FEATURES_DIRECTORY\"\n",
        "subtyping = True\n",
        "results_dir = '/WSI_classification_results'\n",
        "exp_code = 'attempt 1'\n",
        "split_dir = None\n",
        "task = 'task_2_tumor_subtyping'\n",
        "label_frac = 1.0\n",
        "log_data = False\n",
        "max_epochs = 200\n",
        "testing = False\n",
        "bag_loss = 'ce'\n",
        "model_size = 'small' # 'big'\n",
        "early_stopping = False\n",
        "drop_out = False\n",
        "weighted_sample = False\n",
        "no_inst_cluster = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq20Oynz3HKg"
      },
      "source": [
        "seed_torch(seed)\n",
        "\n",
        "settings = {'num_splits': 10, # number of folds\n",
        "            'k_start': 1, # starting fold\n",
        "            'k_end': -1, # ending fold\n",
        "            'task': task,\n",
        "            'max_epochs': max_epochs, \n",
        "            'results_dir': results_dir, \n",
        "            'lr': 1e-4,\n",
        "            'experiment': exp_code,\n",
        "            'reg': 1e-5, # weight decay\n",
        "            'label_frac': label_frac,\n",
        "            'bag_loss': bag_loss,\n",
        "            'seed': seed,\n",
        "            'model_type': model_type,\n",
        "            'model_size': model_size,\n",
        "            \"use_drop_out\": drop_out,\n",
        "            'weighted_sample': weighted_sample,\n",
        "            'opt': 'adam' # ['adam', 'sgd']\n",
        "            }\n",
        "\n",
        "if model_type in ['clam_sb', 'clam_mb']:\n",
        "   settings.update({'bag_weight': bag_weight,\n",
        "                    'inst_loss': inst_loss,\n",
        "                    'B': B})\n",
        "\n",
        "print('\\nLoad Dataset')\n",
        "\n",
        "dataset = Generic_MIL_Dataset(csv_path = 'dataset_csv/tumor_subtyping_dummy_clean.csv',\n",
        "                            data_dir= os.path.join(data_root_dir, 'tumor_subtyping_resnet_features'),\n",
        "                            shuffle = False, \n",
        "                            seed = seed, \n",
        "                            print_info = True,\n",
        "                            label_dict = {'G':0, 'A':1, 'O':2},\n",
        "                            patient_strat= False,\n",
        "                            ignore=[])\n",
        "\n",
        "if model_type in ['clam_sb', 'clam_mb']:\n",
        "  assert subtyping \n",
        "\n",
        "    \n",
        "if not os.path.isdir(results_dir):\n",
        "    os.mkdir(results_dir)\n",
        "\n",
        "results_dir = os.path.join(results_dir, str(exp_code) + '_s{}'.format(seed))\n",
        "if not os.path.isdir(results_dir):\n",
        "    os.mkdir(results_dir)\n",
        "\n",
        "if split_dir is None:\n",
        "    split_dir = os.path.join('splits', task+'_{}'.format(int(label_frac*100)))\n",
        "else:\n",
        "    split_dir = os.path.join('splits', split_dir)\n",
        "\n",
        "print('split_dir: ', split_dir)\n",
        "if not os.path.isdir(split_dir):\n",
        "    os.mkdir(split_dir)\n",
        "\n",
        "settings.update({'split_dir': split_dir})\n",
        "\n",
        "\n",
        "with open(results_dir + '/experiment_{}.txt'.format(exp_code), 'w') as f:\n",
        "    print(settings, file=f)\n",
        "f.close()\n",
        "\n",
        "# print(\"\\n################# Settings ###################\")\n",
        "# for key, val in settings.items():\n",
        "    # print(\"{}:  {}\".format(key, val))        \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "    print(\"finished!\")\n",
        "    print(\"end script\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}