{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_extraction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vimuth97/FYP-Brain-Tumor-Classification/blob/main/Basic%20pathology%20model/feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSWld9Ek5ihx"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "import pdb\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "from torchvision import transforms, utils, models\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import h5py\n",
        "from random import randrange\n",
        "from math import floor\n",
        "import random\n",
        "import time\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "!apt update && apt install -y openslide-tools\n",
        "!pip install openslide-python \n",
        "import openslide \n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmG6LoH1dyu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsiXvrJXD2Tu"
      },
      "source": [
        "def save_hdf5(output_path, asset_dict, attr_dict= None, mode='a'):\n",
        "    file = h5py.File(output_path, mode)\n",
        "    for key, val in asset_dict.items():\n",
        "        data_shape = val.shape\n",
        "        if key not in file:\n",
        "            data_type = val.dtype\n",
        "            chunk_shape = (1, ) + data_shape[1:]\n",
        "            maxshape = (None, ) + data_shape[1:]\n",
        "            dset = file.create_dataset(key, shape=data_shape, maxshape=maxshape, chunks=chunk_shape, dtype=data_type)\n",
        "            dset[:] = val\n",
        "            if attr_dict is not None:\n",
        "                if key in attr_dict.keys():\n",
        "                    for attr_key, attr_val in attr_dict[key].items():\n",
        "                        dset.attrs[attr_key] = attr_val\n",
        "        else:\n",
        "            dset = file[key]\n",
        "            dset.resize(len(dset) + data_shape[0], axis=0)\n",
        "            dset[-data_shape[0]:] = val\n",
        "    file.close()\n",
        "    return output_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXKMky-5rkQ"
      },
      "source": [
        "class Whole_Slide_Bag_FP(Dataset):\n",
        "\tdef __init__(self,\n",
        "\t\tfile_path,\n",
        "\t\twsi,\n",
        "\t\tpretrained=False,\n",
        "\t\tcustom_transforms=None,\n",
        "\t\tcustom_downsample=1,\n",
        "\t\ttarget_patch_size=-1\n",
        "\t\t):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tfile_path (string): Path to the .h5 file containing patched data.\n",
        "\t\t\tpretrained (bool): Use ImageNet transforms\n",
        "\t\t\tcustom_transforms (callable, optional): Optional transform to be applied on a sample\n",
        "\t\t\tcustom_downsample (int): Custom defined downscale factor (overruled by target_patch_size)\n",
        "\t\t\ttarget_patch_size (int): Custom defined image size before embedding\n",
        "\t\t\"\"\"\n",
        "\t\tself.pretrained=pretrained\n",
        "\t\tself.wsi = wsi\n",
        "\t\tif not custom_transforms:\n",
        "\t\t\tself.roi_transforms = eval_transforms(pretrained=pretrained)\n",
        "\t\telse:\n",
        "\t\t\tself.roi_transforms = custom_transforms\n",
        "\n",
        "\t\tself.file_path = file_path\n",
        "\n",
        "\t\twith h5py.File(self.file_path, \"r\") as f:\n",
        "\t\t\tdset = f['coords']\n",
        "\t\t\tself.patch_level = f['coords'].attrs['patch_level']\n",
        "\t\t\tself.patch_size = f['coords'].attrs['patch_size']\n",
        "\t\t\tself.length = len(dset)\n",
        "\t\t\tif target_patch_size > 0:\n",
        "\t\t\t\tself.target_patch_size = (target_patch_size, ) * 2\n",
        "\t\t\telif custom_downsample > 1:\n",
        "\t\t\t\tself.target_patch_size = (self.patch_size // custom_downsample, ) * 2\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.target_patch_size = None\n",
        "\t\tself.summary()\n",
        "\t\t\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.length\n",
        "\n",
        "\tdef summary(self):\n",
        "\t\thdf5_file = h5py.File(self.file_path, \"r\")\n",
        "\t\tdset = hdf5_file['coords']\n",
        "\t\tfor name, value in dset.attrs.items():\n",
        "\t\t\tprint(name, value)\n",
        "\n",
        "\t\tprint('\\nfeature extraction settings')\n",
        "\t\tprint('target patch size: ', self.target_patch_size)\n",
        "\t\tprint('pretrained: ', self.pretrained)\n",
        "\t\tprint('transformations: ', self.roi_transforms)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\twith h5py.File(self.file_path,'r') as hdf5_file:\n",
        "\t\t\tcoord = hdf5_file['coords'][idx]\n",
        "\t\timg = self.wsi.read_region(coord, self.patch_level, (self.patch_size, self.patch_size)).convert('RGB')\n",
        "\n",
        "\t\tif self.target_patch_size is not None:\n",
        "\t\t\timg = img.resize(self.target_patch_size)\n",
        "\t\timg = self.roi_transforms(img).unsqueeze(0)\n",
        "\t\treturn img, coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpMLZBH153_i"
      },
      "source": [
        "class Dataset_All_Bags(Dataset):\n",
        "\n",
        "\tdef __init__(self, csv_path):\n",
        "\t\tself.df = pd.read_csv(csv_path)\n",
        "\t\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.df)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.df['slide_id'][idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bZAbpcM57XE"
      },
      "source": [
        "def print_network(net):\n",
        "\tnum_params = 0\n",
        "\tnum_params_train = 0\n",
        "\tprint(net)\n",
        "\t\n",
        "\tfor param in net.parameters():\n",
        "\t\tn = param.numel()\n",
        "\t\tnum_params += n\n",
        "\t\tif param.requires_grad:\n",
        "\t\t\tnum_params_train += n\n",
        "\t\n",
        "\tprint('Total number of parameters: %d' % num_params)\n",
        "\tprint('Total number of trainable parameters: %d' % num_params_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIGWssxD6hlO"
      },
      "source": [
        "def collate_features(batch):\n",
        "\timg = torch.cat([item[0] for item in batch], dim = 0)\n",
        "\tcoords = np.vstack([item[1] for item in batch])\n",
        "\treturn [img, coords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbFwTT417WZs"
      },
      "source": [
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "class Bottleneck_Baseline(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck_Baseline, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet_Baseline(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet_Baseline, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1) \n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet50_baseline(pretrained=False):\n",
        "    \"\"\"Constructs a Modified ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_Baseline(Bottleneck_Baseline, [3, 4, 6, 3])\n",
        "    if pretrained:\n",
        "        model = load_pretrained_weights(model, 'resnet50')\n",
        "    return model\n",
        "\n",
        "def load_pretrained_weights(model, name):\n",
        "    pretrained_dict = model_zoo.load_url(model_urls[name])\n",
        "    model.load_state_dict(pretrained_dict, strict=False)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIsYIWBVOJPf"
      },
      "source": [
        "def compute_w_loader(file_path, output_path, wsi, model,\n",
        " \tbatch_size = 8, verbose = 0, print_every=20, pretrained=True, \n",
        "\tcustom_downsample=1, target_patch_size=-1):\n",
        "\t\"\"\"\n",
        "\targs:\n",
        "\t\tfile_path: directory of bag (.h5 file)\n",
        "\t\toutput_path: directory to save computed features (.h5 file)\n",
        "\t\tmodel: pytorch model\n",
        "\t\tbatch_size: batch_size for computing features in batches\n",
        "\t\tverbose: level of feedback\n",
        "\t\tpretrained: use weights pretrained on imagenet\n",
        "\t\tcustom_downsample: custom defined downscale factor of image patches\n",
        "\t\ttarget_patch_size: custom defined, rescaled image size before embedding\n",
        "\t\"\"\"\n",
        "\tdataset = Whole_Slide_Bag_FP(file_path=file_path, wsi=wsi, pretrained=pretrained, \n",
        "\t\tcustom_downsample=custom_downsample, target_patch_size=target_patch_size)\n",
        "\tx, y = dataset[0]\n",
        "\tkwargs = {'num_workers': 4, 'pin_memory': True} if device.type == \"cuda\" else {}\n",
        "\tloader = DataLoader(dataset=dataset, batch_size=batch_size, **kwargs, collate_fn=collate_features)\n",
        "\n",
        "\tif verbose > 0:\n",
        "\t\tprint('processing {}: total of {} batches'.format(file_path,len(loader)))\n",
        "\n",
        "\tmode = 'w'\n",
        "\tfor count, (batch, coords) in enumerate(loader):\n",
        "\t\twith torch.no_grad():\t\n",
        "\t\t\tif count % print_every == 0:\n",
        "\t\t\t\tprint('batch {}/{}, {} files processed'.format(count, len(loader), count * batch_size))\n",
        "\t\t\tbatch = batch.to(device, non_blocking=True)\n",
        "\t\t\t\n",
        "\t\t\tfeatures = model(batch)\n",
        "\t\t\tfeatures = features.cpu().numpy()\n",
        "\n",
        "\t\t\tasset_dict = {'features': features, 'coords': coords}\n",
        "\t\t\tsave_hdf5(output_path, asset_dict, attr_dict= None, mode=mode)\n",
        "\t\t\tmode = 'a'\n",
        "\t\n",
        "\treturn output_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyYZuacU5FnG"
      },
      "source": [
        "def eval_transforms(pretrained=False):\n",
        "\tif pretrained:\n",
        "\t\tmean = (0.485, 0.456, 0.406)\n",
        "\t\tstd = (0.229, 0.224, 0.225)\n",
        "\n",
        "\telse:\n",
        "\t\tmean = (0.5,0.5,0.5)\n",
        "\t\tstd = (0.5,0.5,0.5)\n",
        "\n",
        "\ttrnsfrms_val = transforms.Compose(\n",
        "\t\t\t\t\t[\n",
        "\t\t\t\t\t transforms.ToTensor(),\n",
        "\t\t\t\t\t transforms.Normalize(mean = mean, std = std)\n",
        "\t\t\t\t\t]\n",
        "\t\t\t\t)\n",
        "\n",
        "\treturn trnsfrms_val# Gives image names which are already broken into regions\n",
        "def getExtractedImages():\n",
        "    lines = []\n",
        "    path = 'gdrive/My Drive/FYP/RESULTS_DIRECTORY/feature_extracted_images.txt'\n",
        "    \n",
        "\n",
        "\n",
        "    with open(path, 'r') as file:\n",
        "        line = file.readline().strip()\n",
        "        while line != \"\":\n",
        "            lines.append(line)\n",
        "            line = file.readline().strip()\n",
        "    return lines\n",
        "\n",
        "# Update the text file with new image name which is broken into regions\n",
        "def writeExtractedImages(data):\n",
        "    str_data=\"\"\n",
        "    path = 'gdrive/My Drive/FYP/RESULTS_DIRECTORY/feature_extracted_images.txt'\n",
        "    for i in data:\n",
        "        str_data += i+\"\\n\"\n",
        "    with open(path,'w') as file:\n",
        "        file.write(str_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH4nLW7ADla7"
      },
      "source": [
        "def main():\n",
        "\tprint('initializing dataset')\n",
        "\t# feat_dir = 'gdrive/My Drive/FEATURES_DIRECTORY'\n",
        "\t# data_slide_dir = 'gdrive/My Drive/Data_Directory_1' # Located only for 2nd part pre-processed images set only.\n",
        "\t# data_h5_dir = 'gdrive/My Drive/RESULTS_DIRECTORY/rejoined_patches'\n",
        "\t# csv_path = 'gdrive/My Drive/RESULTS_DIRECTORY/Images.csv'\n",
        "\n",
        "\tfeat_dir = 'gdrive/My Drive/FYP/FEATURES_DIRECTORY'\n",
        "\tdata_slide_dir = 'gdrive/My Drive/FYP/Data_Directory' \n",
        "\tdata_h5_dir = 'gdrive/My Drive/FYP/RESULTS_DIRECTORY/rejoined_patches'\n",
        "\tcsv_path = 'gdrive/My Drive/FYP/RESULTS_DIRECTORY/Images.csv'\n",
        "\n",
        "\n",
        "\tslide_ext = '.tiff'\n",
        "\tno_auto_skip = False\n",
        "\tbatch_size = 512\n",
        "\ttarget_patch_size = -1\n",
        "\tcustom_downsample = 1\n",
        "\n",
        "\tif csv_path is None:\n",
        "\t\traise NotImplementedError\n",
        "\n",
        "\tbags_dataset = Dataset_All_Bags(csv_path)\n",
        "\n",
        "\tos.makedirs(feat_dir, exist_ok=True)\n",
        "\tos.makedirs(os.path.join(feat_dir, 'pt_files'), exist_ok=True)\n",
        "\tos.makedirs(os.path.join(feat_dir, 'h5_files'), exist_ok=True)\n",
        "\tdest_files = os.listdir(os.path.join(feat_dir, 'pt_files'))\n",
        "\n",
        "\tprint('loading model checkpoint')\n",
        "\tmodel = resnet50_baseline(pretrained=True)\n",
        "\tmodel = model.to(device)\n",
        "\t\n",
        "\t# print_network(model)\n",
        "\tif torch.cuda.device_count() > 1:\n",
        "\t\tmodel = nn.DataParallel(model)\n",
        "\t\t\n",
        "\tmodel.eval()\n",
        "\ttotal = len(bags_dataset)\n",
        "\textractedImages = getExtractedImages()\n",
        "\tfor bag_candidate_idx in range(total):\n",
        "\t\tslide_id = bags_dataset[bag_candidate_idx].split(slide_ext)[0]\n",
        "\t\tif slide_id in extractedImages:\n",
        "\t\t\tprint('Already Extracted {}'.format(slide_id))\n",
        "\t\t\tcontinue\n",
        "\t\tbag_name = slide_id+'.h5'\n",
        "\t\th5_file_path = os.path.join(data_h5_dir, bag_name)\n",
        "\t\tslide_file_path = os.path.join(data_slide_dir, slide_id+slide_ext)\n",
        "\t\tprint('\\nprogress: {}/{}'.format(bag_candidate_idx, total))\n",
        "\t\tprint(slide_id)\n",
        "\n",
        "\t\tif not no_auto_skip and slide_id+'.pt' in dest_files:\n",
        "\t\t\tprint('skipped {}'.format(slide_id))\n",
        "\t\t\tcontinue \n",
        "\n",
        "\t\toutput_path = os.path.join(feat_dir, 'h5_files', bag_name)\n",
        "\t\ttime_start = time.time()\n",
        "\t\twsi = openslide.open_slide(slide_file_path)\n",
        "\t\toutput_file_path = compute_w_loader(h5_file_path, output_path, wsi,model = model, batch_size = batch_size, verbose = 1, print_every = 20, custom_downsample=custom_downsample, target_patch_size=target_patch_size)\n",
        "\t\ttime_elapsed = time.time() - time_start\n",
        "\t\tprint('\\ncomputing features for {} took {} s'.format(output_file_path, time_elapsed))\n",
        "\t\tfile = h5py.File(output_file_path, \"r\")\n",
        "\n",
        "\t\tfeatures = file['features'][:]\n",
        "\t\tprint('features size: ', features.shape)\n",
        "\t\tprint('coordinates size: ', file['coords'].shape)\n",
        "\t\tfeatures = torch.from_numpy(features)\n",
        "\t\tbag_base, _ = os.path.splitext(bag_name)\n",
        "\t\ttorch.save(features, os.path.join(feat_dir, 'pt_files', bag_base+'.pt'))\n",
        "\t\textractedImages.append(slide_id)\n",
        "\t\twriteExtractedImages(extractedImages)\n",
        "\t\tprint()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvMryZM2hQwO"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQhNZYBc1MmH"
      },
      "source": [
        "Create CSV using Files in Patch folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALV_UqJk1K6l"
      },
      "source": [
        "# Only created for currently patched images only. \n",
        "'''files = os.listdir('gdrive/My Drive/RESULTS_DIRECTORY/rejoined_patches')\n",
        "files\n",
        "files = [name.split(\".\")[0]+\".tiff\" for name in files]\n",
        "df = pd.DataFrame({\"slide_id\":files})\n",
        "df.to_csv('gdrive/My Drive/RESULTS_DIRECTORY/Images.csv')'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpJ9B4yD2pkz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}